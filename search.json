[{"title":"转载 - 零基础入门深度学习(2) - 线性单元和梯度下降","url":"%2F2019%2F05%2F03%2Fdeeplearning-quickstart-2%2F","content":"\n![](/static/upload-images.jianshu.io/upload_images/2256672-06627c71f0d8c0dc.jpg)\n\n> 无论即将到来的是大数据时代还是人工智能时代，亦或是传统行业使用人工智能在云上处理大数据的时代，作为一个有理想有追求的程序员，\n不懂深度学习（Deep Learning）这个超热的技术，会不会感觉马上就out了？现在救命稻草来了，《零基础入门深度学习》\n系列文章旨在讲帮助爱编程的你从零基础达到入门级水平。零基础意味着你不需要太多的数学知识，只要会写程序就行了，没错，\n这是专门为程序员写的文章。虽然文中会有很多公式你也许看不懂，但同时也会有更多的代码，程序员的你一定能看懂的\n（我周围是一群狂热的Clean Code程序员，所以我写的代码也不会很差）。\n\n## 往期回顾\n\n在上一篇文章中，我们已经学会了编写一个简单的感知器，并用它来实现一个线性分类器。你应该还记得用来训练感知器的『感知器规则』。\n然而，我们并没有关心这个规则是怎么得到的。本文通过介绍另外一种『感知器』，也就是『线性单元』，来说明关于机器学习一些基本的概念，\n比如模型、目标函数、优化算法等等。这些概念对于所有的机器学习算法来说都是通用的，掌握了这些概念，就掌握了机器学习的基本套路。\n\n## 线性单元是啥\n\n<!-- more -->\n\n感知器有一个问题，当面对的数据集不是 **线性可分** 的时候，『感知器规则』可能无法收敛，\n这意味着我们永远也无法完成一个感知器的训练。为了解决这个问题，我们使用一个 **可导** 的 **线性函数** 来替代感知器的 **阶跃函数** ，\n这种感知器就叫做 **线性单元** 。线性单元在面对线性不可分的数据集时，会收敛到一个最佳的近似上。\n\n为了简单起见，我们可以设置线性单元的激活函数 $f$ 为\n\n$$\n    f(x) = x\n$$\n\n这样的线性单元如下图所示\n\n![](/static/upload-images.jianshu.io/upload_images/2256672-f57602e423d739ee.png)\n\n对比此前我们讲过的感知器\n\n![](/static/upload-images.jianshu.io/upload_images/2256672-801d65e79bfc3162.png)\n\n这样替换了激活函数 $f$ 之后， **线性单元** 将返回一个 **实数值** 而不是 **0,1分类** 。因此线性单元用来解决 **回归** 问题而不是 **分类** 问题。\n\n### 线性单元的模型\n\n当我们说 **模型** 时，我们实际上在谈论根据输入 $x$ 预测输出 $y$ 的 **算法** 。比如， $x$ 可以是一个人的工作年限， $y$ 可以是他的月薪，我们可以用某种算法来根据一个人的工作年限来预测他的收入。比如：\n\n$$\n    y = h(x) = w * x + b\n$$\n\n函数 $h(x)$ 叫做 **假设** ，而 $w$ 、 $b$ 是它的 **参数** 。我们假设参数 $w = 1000$ ，参数 $b = 500$ ，如果一个人的工作年限是5年的话，我们的模型会预测他的月薪为\n\n$$\n    y = h(x) = 1000 * 5 + 500 = 5500(元)\n$$\n\n你也许会说，这个模型太不靠谱了。是这样的，因为我们考虑的因素太少了，仅仅包含了工作年限。如果考虑更多的因素，比如所处的行业、公司、职级等等，可能预测就会靠谱的多。我们把工作年限、行业、公司、职级这些信息，称之为 **特征** 。对于一个工作了5年，在IT行业，百度工作，职级T6这样的人，我们可以用这样的一个特征向量来表示他 $\\mathrm{x} = (5, IT, 百度, T6)$ 。\n\n既然输入 $\\mathrm{x}$ 变成了一个具备四个特征的向量，相对应的，仅仅一个参数 $w$ 就不够用了，我们应该使用4个参数 $w_1, w_2, w_3, w_4$ ，每个特征对应一个。这样，我们的模型就变成\n\n$$\n    y = h(x) = w_1 * x_1 + w_2 * x_2 + w_3 * x_3 + w_4 * x_4 + b\n$$\n\n其中， $x_1$ 对应工作年限， $x_2$ 对应行业， $x_3$ 对应公司， $x_4$ 对应职级。\n\n为了书写和计算方便，我们可以令 $w_0$ 等于 $b$ ，同时令 $w_0$ 对应于特征 $x_0$ 。由于 $x_0$ 其实并不存在，我们可以令它的值永远为1。也就是说\n\n$$\n    b = w_0 * x_0, 其中 x_0 = 1\n$$\n\n这样上面的式子就可以写成\n\n$$\n\\begin{aligned}\n    y &= h(x) = w_1 * x_1 + w_2 * x_2 + w_3 * x_3 + w_4 * x_4 + b &\\pod{1} \\\\\n    & = w_0 * x_0 + w_1 * x_1 + w_2 * x_2 + w_3 * x_3 + w_4 * x_4 &\\pod{2}\n\\end{aligned}\n$$\n\n我们还可以把上式写成向量的形式\n\n$$\n    y = h(x) = \\mathrm{w}^\\mathrm{T} \\mathrm{x} \\tag{式 1}\n$$\n\n长成这种样子模型就叫做 **线性模型** ，因为输出 $y$ 就是输入特 $x_1, x_2, x_3, \\dots$ 征的 **线性组合** 。\n\n### 监督学习和无监督学习\n\n接下来，我们需要关心的是这个模型如何训练，也就是参数 $w$ 取什么值最合适。\n\n机器学习有一类学习方法叫做 **监督学习** ，它是说为了训练一个模型，我们要提供这样一堆训练样本：每个训练样本既包括输入特征 $\\mathrm{x}$ ，也包括对应的输出 $y$ ( $y$ 也叫做 **标记** ， **label** )。也就是说，我们要找到很多人，我们既知道他们的特征(工作年限，行业...)，也知道他们的收入。我们用这样的样本去训练模型，让模型既看到我们提出的每个问题(输入特征 $\\mathrm{x}$ )，也看到对应问题的答案(标记 $y$ )。当模型看到足够多的样本之后，它就能总结出其中的一些规律。然后，就可以预测那些它没看过的输入所对应的答案了。\n\n另外一类学习方法叫做 **无监督学习** ，这种方法的训练样本中只有 $\\mathrm{x}$ 而没有 $y$ 。模型可以总结出特征的一些规律，但是无法知道其对应的答案 $y$ 。\n\n很多时候，既有 $\\mathrm{x}$ 又有 $y$ 的训练样本是很少的，大部分样本都只有 $\\mathrm{x}$ 。比如在语音到文本(STT)的识别任务中， $\\mathrm{x}$ 是语音， $y$ 是这段语音对应的文本。我们很容易获取大量的语音录音，然而把语音一段一段切分好并 **标注** 上对应文字则是非常费力气的事情。这种情况下，为了弥补带标注样本的不足，我们可以用 **无监督学习方法** 先做一些 **聚类** ，让模型总结出哪些音节是相似的，然后再用少量的带标注的训练样本，告诉模型其中一些音节对应的文字。这样模型就可以把相似的音节都对应到相应文字上，完成模型的训练。\n\n### 线性单元的目标函数\n\n现在，让我们只考虑 **监督学习** 。\n\n在监督学习下，对于一个样本，我们知道它的特征 $\\mathrm{x}$ ，以及标记 $y$ 。同时，我们还可以根据模型 $h(x)$ 计算得到输出 $\\bar{y}$ 。注意这里面我们用 $y$ 表示训练样本里面的 **标记** ，也就是实际值；用带上划线的 $\\bar{y}$ 表示模型计算的出来的 **预测值** 。我们当然希望模型计算出来的 $\\bar{y}$ 和 $y$ 越接近越好。\n\n数学上有很多方法来表示 $\\bar{y}$ 和 $y$ 接近程度，比如我们可以用 $\\bar{y}$ 和 $y$ 的差的平方的来表示它们的接近程度\n\n$$\ne = \\frac{1}{2}(y - \\bar{y})^2\n$$\n\n我们把 $e$ 叫做 **单个样本的误差** 。至于为什么前面要乘 $\\frac{1}{2}$ ，是为了后面计算方便。\n\n训练数据中会有很多样本，比如 $N$ 个，我们可以用训练数据中 **所有样本** 的误差的 **和** ，来表示模型的误差 $E$ ，也就是\n\n$$\nE = e^{(1)} + e^{(2)} + e^{(3)} + \\dots + e^{(n)}\n$$\n\n上式的 $e^{(1)}$ 表示第一个样本的误差， $e^{(2)}$ 表示第二个样本的误差......。\n\n我们还可以把上面的式子写成和式的形式。使用和式，不光书写起来简单，逼格也跟着暴涨，一举两得。所以一定要写成下面这样\n\n$$\n\\begin{aligned}\n    E &= e^{(1)} + e^{(2)} + e^{(3)} + \\dots + e^{(n)} &\\pod{3} \\\\\n    &= \\sum \\limits_{i=0}^{n} e^{(i)} &\\pod{4} \\\\\n    &= \\frac{1}{2} \\sum \\limits_{i=0}^{n} (y^{(i)} - \\bar{y}^{(i)})^2 \\pod{式2} &\\pod{5}\n\\end{aligned}\n$$\n\n其中\n\n$$\n\\begin{aligned}\n    \\bar{y}^{(i)} &= h(x^{(i)}) &\\pod{6} \\\\\n    &=\\mathrm{w}^T\\mathrm{x}^{(i)} &\\pod{7}\n\\end{aligned}\n$$\n\n(式2)中， $x^{(i)}$ 表示第 $i$ 个训练样本的 **特征** ， $y^{(i)}$ 表示第 $i$ 个样本的 **标记** ，我们也可以用元组 $(x^{(i)}, y^{(i)})$ 表示第 $i$ 个 **训练样本** 。 $\\bar{y}^{(i)}$ 则是模型对第 $i$ 个样本的 **预测值** 。\n\n我们当然希望对于一个训练数据集来说，误差最小越好，也就是(式2)的值越小越好。对于特定的训练数据集来说， $(x^{(i)}, y^{(i)})$ 的值都是已知的，所以(式2)其实是参数 $\\mathrm{w}$ 的函数。\n\n$$\n\\begin{aligned}\n    E &= \\frac{1}{2} \\sum \\limits_{i=0}^{n} (y^{(i)} - \\bar{y}^{(i)})^2 &\\pod{8} \\\\\n    &= \\frac{1}{2} \\sum \\limits_{i=0}^{n} (y^{(i)} - \\mathrm{w}^T\\mathrm{x}^{(i)})^2 &\\pod{9}\n\\end{aligned}\n$$\n\n由此可见，模型的训练，实际上就是求取到合适的 $\\mathrm{w}$ ，使(式2)取得最小值。这在数学上称作 **优化问题** ，而 $E(\\mathrm{w})$ 就是我们优化的目标，称之为 **目标函数** 。\n\n### 梯度下降优化算法\n\n大学时我们学过怎样求函数的极值。函数 $y= f(x)$ 的极值点，就是它的导数 $f'(x) = 0$ 的那个点。因此我们可以通过解方程 $f'(x) = 0$ ，求得函数的极值点 $(x_0, y_0)$ 。\n\n不过对于计算机来说，它可不会解方程。但是它可以凭借强大的计算能力，一步一步的去把函数的极值点『试』出来。如下图所示：\n\n![](/static/upload-images.jianshu.io/upload_images/2256672-46acc2c2d52fc366.png)\n\n首先，我们随便选择一个点开始，比如上图的 $x_0$ 点。接下来，每次迭代修改 $x$ 为 $x_1, x_2, x_3, \\dots$ ，经过数次迭代后最终达到函数最小值点。\n\n你可能要问了，为啥每次修改 $x$ 的值，都能往函数最小值那个方向前进呢？这里的奥秘在于，我们每次都是向函数 $y = f(x)$ 的 **梯度** 的 **相反方向** 来修改 $x$ 。什么是 **梯度** 呢？翻开大学高数课的课本，我们会发现 **梯度** 是一个向量，它指向 **函数值上升最快** 的方向。显然，梯度的反方向当然就是函数值下降最快的方向了。我们每次沿着梯度相反方向去修改 $x$ 的值，当然就能走到函数的最小值附近。之所以是最小值附近而不是最小值那个点，是因为我们每次移动的步长不会那么恰到好处，有可能最后一次迭代走远了越过了最小值那个点。步长的选择是门手艺，如果选择小了，那么就会迭代很多轮才能走到最小值附近；如果选择大了，那可能就会越过最小值很远，收敛不到一个好的点上。\n\n按照上面的讨论，我们就可以写出梯度下降算法的公式\n\n$$\nx_{new} = x_{old} - \\eta \\nabla f(x)\n$$\n\n其中， $\\nabla$ 是 **梯度算子** ， $\\nabla f(x)$ 就是指的 $f(x) 梯度。 $\\eta$ 是步长，也称作 **学习速率** 。\n\n对于上一节列出的目标函数(式2)\n\n$$\nE(\\mathrm{w}) = \\frac{1}{2} \\sum \\limits_{i=1}^{n} (y^{i} - \\bar{y}^{(i)})^2\n$$\n\n梯度下降算法可以写成\n\n$$\n\\mathrm{w}_{new} = \\mathrm{w}_{old} - \\eta \\nabla E(\\mathrm{w})\n$$\n\n聪明的你应该能想到，如果要求目标函数的 **最大值** ，那么我们就应该用梯度上升算法，它的参数修改规则是\n\n$$\n\\mathrm{w}_{new} = \\mathrm{w}_{old} + \\eta \\nabla E(\\mathrm{w})\n$$\n\n下面，请先做几次深呼吸，让你的大脑补充足够的新鲜的氧气，我们要来求取 $\\nabla E(\\mathrm{w})$ ，然后带入上式，就能得到线性单元的参数修改规则。\n\n关于 $\\nabla E(\\mathrm{w})$ 的推导过程，我单独把它们放到一节中。您既可以选择慢慢看，也可以选择无视。在这里，您只需要知道，经过一大串推导，目标函数 $E(\\mathrm{w})$ 的梯度是\n\n$$\n\\nabla E(\\mathrm{w}) = - \\sum \\limits_{i=1}^{n} (y^{(i)} - \\bar{y}^{(i)}) \\mathrm{x}^{(i)}\n$$\n\n因此，线性单元的参数修改规则最后是这个样子\n\n$$\n\\mathrm{w}_{new} = \\mathrm{w}_{old} + \\eta \\sum \\limits_{i=1}^{n} (y^{(i)} - \\bar{y}^{(i)}) \\mathrm{x}^{(i)} \\pod{式3}\n$$\n\n有了上面这个式子，我们就可以根据它来写出训练线性单元的代码了。\n\n需要说明的是，如果每个样本有M个特征，则上式中的 $\\mathrm{x}, \\mathrm{w}$ 都是M+1维 **向量** (因为我们加上了一个恒为1的虚拟特征 $x_0$，参考前面的内容)，而 $y$ 是 **标量** 。用高逼格的数学符号表示，就是\n\n$$\n\\mathrm{x}, \\mathrm{w} \\in \\mathrm{\\Re}^{M+1} \\\\\ny \\in \\Re^1\n$$\n\n为了让您看明白说的是啥，我吐血写下下面这个解释(写这种公式可累可累了)。因为 $\\mathrm{w}, \\mathrm{x}$ 是M+1维 **列向量** ，所以(式3)可以写成\n\n$$\n\\begin{bmatrix}\n    w_0 \\\\ w_1 \\\\ w_2 \\\\ \\dots \\\\ w_m\n\\end{bmatrix}_{new}\n=\n\\begin{bmatrix}\n    w_0 \\\\ w_1 \\\\ w_2 \\\\ \\dots \\\\ w_m\n\\end{bmatrix}_{old}\n+ \\eta \\sum \\limits_{i=1}^{n} (y^{(i)} - \\bar{y}^{(i)})\n\\begin{bmatrix}\n    1 \\\\ x_{1}^{(i)} \\\\ x_{2}^{(i)} \\\\ \\dots \\\\ x_{m}^{(i)}\n\\end{bmatrix}\n$$\n\n如果您还是没看明白，建议您也吐血再看一下大学时学过的《线性代数》吧。\n\n### $\\nabla E(\\mathrm{w})$ 的推导\n\n这一节你尽可以跳过它，并不太会影响到全文的理解。当然如果你非要弄明白每个细节，那恭喜你骚年，机器学习的未来一定是属于你的。\n\n首先，我们先做一个简单的前戏。我们知道函数的梯度的定义就是它相对于各个变量的 **偏导数** ，所以我们写下下面\n的式子\n\n$$\n\\begin{aligned}\n    \\nabla E(\\mathrm{w})\n    &= \\frac{\\partial}{\\partial \\mathrm{w}} E(\\mathrm{w}) &\\pod{10} \\\\\n    &= \\frac{\\partial}{\\partial \\mathrm{w}} \\frac{1}{2} \\sum \\limits_{i=1}^{n} (y^{(i)} -\\bar{y}^{(i)})^2 &\\pod{11}\n\n\\end{aligned}\n$$\n\n可接下来怎么办呢？我们知道和的导数等于导数的和，所以我们可以先把求和符号 $\\sum$ 里面的导数求出来，然后再把它们加在一起就行了，也就是\n\n$$\n\\begin{aligned}\n    \\frac{\\partial}{\\partial \\mathrm{w}} \\frac{1}{2} \\sum \\limits_{i=1}^{n} (y^{(i)} -\\bar{y}^{(i)})^2 &\\pod{12} \\\\\n    = \\frac{1}{2} \\sum \\limits_{i=1}^{n} \\frac{\\partial}{\\partial \\mathrm{w}} (y^{(i)} -\\bar{y}^{(i)})^2 &\\pod{13}\n\\end{aligned}\n$$\n\n现在我们可以不管高大上的 $\\sum$ 了，先专心把里面的导数求出来。\n\n$$\n\\begin{aligned}\n    \\frac{\\partial}{\\partial \\mathrm{w}} (y^{(i)} -\\bar{y}^{(i)})^2 &\\pod{14} \\\\\n    = \\frac{\\partial}{\\partial \\mathrm{w}} (y^{(i)2} - 2\\bar{y}^{(i)} y^{(i)} + \\bar{y}^{(i)2}) &\\pod{15}\n\\end{aligned}\n$$\n\n我们知道， $y$ 是与 $\\mathrm{w}$ 无关的常数，而 $\\bar{y} = \\mathrm{w}^T \\mathrm{x}$ ，下面我们根据链式求导法则来求导(上大学时好像叫复合函数求导法则)\n\n$$\n\\frac{\\partial E(\\mathrm{w})}{\\partial \\mathrm{w}} = \\frac{\\partial E(\\bar{y})}{\\partial \\bar{y}} \\frac{\\partial \\bar{y}}{\\partial \\mathrm{w}}\n$$\n\n我们分别计算上式等号右边的两个偏导数\n\n$$\n\\begin{aligned}\n    \\frac{\\partial E(\\mathrm{w})}{\\partial \\bar{y}}\n    &= \\frac{\\partial}{\\partial \\bar{y}} (y^{(i)2} - 2\\bar{y}^{(i)} y^{(i)} + \\bar{y}^{(i)2}) &\\pod{16} \\\\\n    &= -2y^{(i)} + 2\\bar{y}^{(i)} &\\pod{17}\n\\end{aligned}\n$$\n\n>注: 此处原文应该是作者公式错误多写了一个方程编号, 不影响后续, 移除编号 **(18)**\n\n$$\n\\begin{aligned}\n    \\frac{\\partial \\bar{y}}{\\partial \\mathrm{w}}\n    &= \\frac{\\partial}{\\partial \\mathrm{w}} \\mathrm{w}^T \\mathrm{x} &\\pod{19} \\\\\n    &= \\mathrm{x} &\\pod{20}\n\\end{aligned}\n$$\n\n代入，我们求得 $\\sum$ 里面的偏导数是\n\n$$\n\\begin{aligned}\n    \\frac{\\partial}{\\partial \\mathrm{w}} (y^{(i)} -\\bar{y}^{(i)})^2 &\\pod{21} \\\\\n    = 2(-y^{(i) + \\bar{y}^{(i)}})\\mathrm{x} &\\pod{22}\n\\end{aligned}\n$$\n\n最后代入 $\\nabla E(\\mathrm{w})$ ，求得\n\n$$\n\\begin{aligned}\n    \\nabla E(\\mathrm{w})\n    &= \\frac{1}{2} \\sum \\limits_{i=1}^{n} \\frac{\\partial}{\\partial \\mathrm{w}} (y^{(i)} -\\bar{y}^{(i)})^2 &\\pod{23} \\\\\n    &= \\frac{1}{2} \\sum \\limits_{i=1}^{n} \\frac{\\partial}{\\partial \\mathrm{w}} 2(-y^{(i)} +\\bar{y}^{(i)}) \\mathrm{x} &\\pod{24} \\\\\n    &= - \\sum \\limits_{i=1}^{n} (y^{(i)} - \\bar{y}^{(i)}) \\mathrm{x} &\\pod{25}\n\\end{aligned}\n$$\n\n至此，大功告成。\n\n### 随机梯度下降算法(Stochastic Gradient Descent, SGD)\n\n如果我们根据(式3)来训练模型，那么我们每次更新 $\\mathrm{w}$ 的迭代，要遍历训练数据中所有的样本进行计算，我们称这种算法叫做 **批梯度下降(Batch Gradient Descent)** 。如果我们的样本非常大，比如数百万到数亿，那么计算量异常巨大。因此，实用的算法是SGD算法。在SGD算法中，每次更新 $\\mathrm{w}$ 的迭代，只计算一个样本。这样对于一个具有数百万样本的训练数据，完成一次遍历就会对 $\\mathrm{w}$ 更新数百万次，效率大大提升。由于样本的噪音和随机性，每次更新 $\\mathrm{w}$ 并不一定按照减少 $E$ 的方向。然而，虽然存在一定随机性，大量的更新总体上沿着减少 $E$ 的方向前进的，因此最后也能收敛到最小值附近。下图展示了SGD和BGD的区别\n\n![](/static/upload-images.jianshu.io/upload_images/2256672-3152002d503d768e.png)\n\n如上图，椭圆表示的是函数值的等高线，椭圆中心是函数的最小值点。红色是BGD的逼近曲线，而紫色是SGD的逼近曲线。我们可以看到BGD是一直向着最低点前进的，而SGD明显躁动了许多，但总体上仍然是向最低点逼近的。\n\n最后需要说明的是，SGD不仅仅效率高，而且随机性有时候反而是好事。今天的目标函数是一个『凸函数』，沿着梯度反方向就能找到全局唯一的最小值。然而对于非凸函数来说，存在许多局部最小值。随机性有助于我们逃离某些很糟糕的局部最小值，从而获得一个更好的模型。\n\n## 实现线性单元\n\n>完整代码请参考GitHub: https://github.com/hanbt/learn_dl/blob/master/linear_unit.py (python2.7)\n\n接下来，让我们撸一把代码。\n\n因为我们已经写了感知器的代码，因此我们先比较一下感知器模型和线性单元模型，看看哪些代码能够复用。\n\n| **算法** | **感知器** | **线性单元** |\n| ----- | ----- | --- |\n| 模型 $h(x)$ | $y=f(\\mathrm{w}^T \\mathrm{x})$ | $y=f(\\mathrm{w}^T \\mathrm{x})$ |\n||$f(z)=\\begin{cases} 1 \\quad z \\gt 0 \\\\ 0 \\quad otherwise \\end{cases}$| $f(z)=z$ |\n| 训练规则 | $\\mathrm{w} \\leftarrow \\mathrm{w} + \\eta (y-\\bar{y})\\mathrm{x}$ | $\\mathrm{w} \\leftarrow \\mathrm{w} + \\eta (y-\\bar{y})\\mathrm{x}$ |\n\n比较的结果令人震惊，原来除了激活函数不同之外，两者的模型和训练规则是一样的(在上表中，线性单元的优化算法是SGD算法)。那么，我们只需要把感知器的激活函数进行替换即可。感知器的代码请参考上一篇文章[零基础入门深度学习(1) - 感知器](/2019/05/01/deeplearning-quickstart-1/)，这里就不再重复了。对于一个养成良好习惯的程序员来说，重复代码是不可忍受的。大家应该把代码保存在一个代码库中(比如git)。\n\n```python\nfrom perceptron import Perceptron\n#定义激活函数f\nf = lambda x: x\nclass LinearUnit(Perceptron):\n    def __init__(self, input_num):\n        '''初始化线性单元，设置输入参数的个数'''\n        Perceptron.__init__(self, input_num, f)\n```\n\n通过继承Perceptron，我们仅用几行代码就实现了线性单元。这再次证明了面向对象编程范式的强大。\n\n接下来，我们用简单的数据进行一下测试。\n\n```python\ndef get_training_dataset():\n    '''\n    捏造5个人的收入数据\n    '''\n    # 构建训练数据\n    # 输入向量列表，每一项是工作年限\n    input_vecs = [[5], [3], [8], [1.4], [10.1]]\n    # 期望的输出列表，月薪，注意要与输入一一对应\n    labels = [5500, 2300, 7600, 1800, 11400]\n    return input_vecs, labels    \ndef train_linear_unit():\n    '''\n    使用数据训练线性单元\n    '''\n    # 创建感知器，输入参数的特征数为1（工作年限）\n    lu = LinearUnit(1)\n    # 训练，迭代10轮, 学习速率为0.01\n    input_vecs, labels = get_training_dataset()\n    lu.train(input_vecs, labels, 10, 0.01)\n    #返回训练好的线性单元\n    return lu\nif __name__ == '__main__': \n    '''训练线性单元'''\n    linear_unit = train_linear_unit()\n    # 打印训练获得的权重\n    print linear_unit\n    # 测试\n    print 'Work 3.4 years, monthly salary = %.2f' % linear_unit.predict([3.4])\n    print 'Work 15 years, monthly salary = %.2f' % linear_unit.predict([15])\n    print 'Work 1.5 years, monthly salary = %.2f' % linear_unit.predict([1.5])\n    print 'Work 6.3 years, monthly salary = %.2f' % linear_unit.predict([6.3])\n```\n\n程序运行结果如下图\n\n![](/static/upload-images.jianshu.io/upload_images/2256672-92f00082e3db32d2.png)\n\n拟合的直线如下图\n\n![](/static/upload-images.jianshu.io/upload_images/2256672-b2db886ef2f18771.png)\n\n## 小结\n\n事实上，一个机器学习算法其实只有两部分\n\n- *模型* 从输入特征 $\\mathrm{x}$ 预测输入 $y$ 的那个函数 $h(x)$\n- *目标函数* 目标函数取最小(最大)值时所对应的参数值，就是模型的参数的 **最优值** 。很多时候我们只能获得目标函数的 **局部最小(最大)值** ，因此也只能得到模型参数的局部最优值。\n\n因此，如果你想最简洁的介绍一个算法，列出这两个函数就行了。\n\n接下来，你会用 **优化算法** 去求取目标函数的最小(最大)值。**[随机]梯度{下降|上升}** 算法就是一个 **优化算法** 。针对同一个目标函数，不同的 **优化算法** 会推导出不同的训练规则。我们后面还会讲其它的优化算法。\n\n其实在机器学习中，算法往往并不是关键，真正的关键之处在于选取特征。选取特征需要我们人类对问题的深刻理解，经验、以及思考。而 **神经网络** 算法的一个优势，就在于它能够自动学习到应该提取什么特征，从而使算法不再那么依赖人类，而这也是神经网络之所以吸引人的一个方面。\n\n现在，经过漫长的烧脑，你已经具备了学习 **神经网络** 的必备知识。下一篇文章，我们将介绍本系列文章的主角： **神经网络** ，以及用来训练神经网络的大名鼎鼎的算法： **反向传播** 算法。至于现在，我们应该暂时忘记一切，尽情奖励自己一下吧。\n\n> 注: 去掉原作者爱吃的红烧肉 ->_->\n\n## 参考资料\n\n1. Tom M. Mitchell, \"机器学习\", 曾华军等译, 机械工业出版社","tags":["深度学习入门"],"categories":["转载"]},{"title":"零基础入门深度学习 - Menu","url":"%2F2019%2F05%2F01%2Fdeeplearning-quickstart-menu%2F","content":"\n> 本系列文章全文转载自作业部落 [@hanbingtao](https://www.zybuluo.com/hanbingtao/note/433855) 老师的零基础入门系列文章.\n由于作业部落没有很好的索引相关功能, 转载于个人博客, 重新做了排版工作, 版权归原作者所有.\n\n## 文章列表\n\n-  <a href=\"/2019/05/01/deeplearning-quickstart-1/\" target=\"_blank\">零基础入门深度学习(1) - 感知器</a>\n\n-  <a href=\"/2019/05/03/deeplearning-quickstart-2/\" target=\"_blank\">零基础入门深度学习(2) - 线性单元和梯度下降</a>\n\n- 零基础入门深度学习(3) - 神经网络和反向传播算法\n\n- 零基础入门深度学习(4) - 卷积神经网络\n\n- 零基础入门深度学习(5) - 循环神经网络\n\n- 零基础入门深度学习(6) - 长短时记忆网络(LSTM)\n\n- 零基础入门深度学习(7) - 递归神经网络\n\n","tags":["深度学习入门"]},{"title":"转载 - 零基础入门深度学习(1) - 感知器","url":"%2F2019%2F05%2F01%2Fdeeplearning-quickstart-1%2F","content":"\n![](/static/upload-images.jianshu.io/upload_images/2256672-06627c71f0d8c0dc.jpg)\n\n> 无论即将到来的是大数据时代还是人工智能时代，亦或是传统行业使用人工智能在云上处理大数据的时代，作为一个有理想有追求的程序员，不懂深度学习（Deep Learning）这个超热的技术，会不会感觉马上就out了？现在救命稻草来了，《零基础入门深度学习》系列文章旨在讲帮助爱编程的你从零基础达到入门级水平。零基础意味着你不需要太多的数学知识，只要会写程序就行了，没错，这是专门为程序员写的文章。虽然文中会有很多公式你也许看不懂，但同时也会有更多的代码，程序员的你一定能看懂的（我周围是一群狂热的Clean Code程序员，所以我写的代码也不会很差）。\n\n## 深度学习是啥\n\n在人工智能领域，有一个方法叫机器学习。在机器学习这个方法里，有一类算法叫神经网络。神经网络如下图所示：\n\n<!-- more -->\n\n![](/static/upload-images.jianshu.io/upload_images/2256672-c6f640c11a06ac2e.png)\n\n上图中每个圆圈都是一个神经元，每条线表示神经元之间的连接。我们可以看到，上面的神经元被分成了多层，层与层之间的神经元有连接，而层内之间的神经元没有连接。最左边的层叫做 **输入层** ，这层负责接收输入数据；最右边的层叫 **输出层** ，我们可以从这层获取神经网络输出数据。输入层和输出层之间的层叫做 **隐藏层** 。\n\n隐藏层比较多（大于2）的神经网络叫做深度神经网络。而深度学习，就是使用深层架构（比如，深度神经网络）的机器学习方法。\n\n那么深层网络和浅层网络相比有什么优势呢？简单来说深层网络能够表达力更强。事实上，一个仅有一个隐藏层的神经网络就能拟合任何一个函数，但是它需要很多很多的神经元。而深层网络用少得多的神经元就能拟合同样的函数。也就是为了拟合一个函数，要么使用一个浅而宽的网络，要么使用一个深而窄的网络。而后者往往更节约资源。\n\n深层网络也有劣势，就是它不太容易训练。简单的说，你需要大量的数据，很多的技巧才能训练好一个深层网络。这是个手艺活。\n\n## 感知器\n\n看到这里，如果你还是一头雾水，那也是很正常的。为了理解神经网络，我们应该先理解神经网络的组成单元—— **神经元** 。神经元也叫做 **感知器** 。感知器算法在上个世纪50-70年代很流行，也成功解决了很多问题。并且，感知器算法也是非常简单的。\n\n### 感知器的定义\n\n下图是一个感知器：\n\n![](/static/upload-images.jianshu.io/upload_images/2256672-801d65e79bfc3162.png)\n\n可以看到，一个感知器有如下组成部分：\n\n- **输入权值** 一个感知器可以接收多个输入 $(x_1, x_2, \\dots, x_n | x_i \\in \\Re)$，每个输入上有一个 **权值** $w_i \\in \\Re$，此外还有一个 **偏置项** $b \\in \\Re$ ，就是上图中的 $w_0$ 。\n\n- **激活函数** 感知器的激活函数可以有很多选择，比如我们可以选择下面这个**阶跃函数** $f$ 来作为激活函数：\n\n$$\nf(z)=\\begin{cases}\n    1 \\quad z\\gt 0 \\\\\n    \\\\\n    0 \\quad othercase \\\\\n\\end{cases} \\tag{1}\n$$\n\n- **输出** 感知器的输出由下面这个公式来计算\n\n$$\n    y = f(w \\bullet x + b) \\tag{公式1}\n$$\n\n如果看完上面的公式一下子就晕了，不要紧，我们用一个简单的例子来帮助理解。\n\n#### 例子：用感知器实现`and`函数\n\n我们设计一个感知器，让它来实现`and`运算。程序员都知道，`and`是一个二元函数（带有两个参数 $x_1$ 和 $x_2$），下面是它的 **真值表**：\n\n| $x_1$ | $x_2$ | $y$ |\n| ----- | ----- | --- |\n|0|0|0|\n|0|1|0|\n|1|0|0|\n|1|1|1|\n\n为了计算方便，我们用0表示 **false** ，用1表示 **true** 。这没什么难理解的，对于C语言程序员来说，这是天经地义的。\n\n我们令 $w_1 = 0.5; w_2 = 0.5; b = -0.8$，而激活函数 $f$ 就是前面写出来的 **阶跃函数** ，这时，感知器就相当于`and`函数。不明白？我们验算一下：\n\n输入上面真值表的第一行，即$x_1 = 0; x_2 = 0$，那么根据公式(1)，计算输出： \n\n$$\n\\begin{aligned}\n    y &= f(w \\bullet x + b) &\\pod{2} \\\\\n    &= f(w_1 x_1 + w_2 x_2 + b ) &\\pod{3} \\\\\n    &= f(0.5 \\times 0 + 0.5 \\times 0 - 0.8) &\\pod{4} \\\\\n    &= f(-0.8) &\\pod{5} \\\\\n    &= 0 &\\pod{6}\n\\end{aligned}\n$$\n\n也就是当 $x_1 x_2$ 都为0的时候，$y$ 为0，这就是 **真值表** 的第一行。读者可以自行验证上述真值表的第二、三、四行。\n\n#### 例子：用感知器实现`or`函数\n\n同样，我们也可以用感知器来实现`or`运算。仅仅需要把偏置项 $b$ 的值设置为-0.3就可以了。我们验算一下，下面是`or`运算的 **真值表** ：\n\n| $x_1$ | $x_2$ | $y$ |\n| ----- | ----- | --- |\n|0|0|0|\n|0|1|1|\n|1|0|1|\n|1|1|1|\n\n我们来验算第二行，这时的输入是 $x_1 = 0; x_2 = 1$ ，带入公式(1)：\n\n$$\n\\begin{aligned}\n    y &= f(w \\bullet x + b) &\\pod{7} \\\\\n    &= f(w_1 x_1 + w_2 x_2 + b ) &\\pod{8} \\\\\n    &= f(0.5 \\times 0 + 0.5 \\times 1 - 0.3) &\\pod{9} \\\\\n    &= f(0.2) &\\pod{10} \\\\\n    &= 1 &\\pod{11}\n\\end{aligned}\n$$\n\n也就是当 $x_1 = 0; x_2 = 1$ 时， $y$ 为1，即`or` **真值表** 第二行。读者可以自行验证其它行。\n\n### 感知器还能做什么\n\n事实上，感知器不仅仅能实现简单的布尔运算。它可以拟合任何的线性函数，任何 **线性分类** 或 **线性回归** 问题都可以用感知器来解决。前面的布尔运算可以看作是 **二分类** 问题，即给定一个输入，输出0（属于分类0）或1（属于分类1）。如下面所示，`and`运算是一个线性分类问题，即可以用一条直线把分类0（false，红叉表示）和分类1（true，绿点表示）分开。\n\n![](/static/upload-images.jianshu.io/upload_images/2256672-acff576747ef4259.png)\n\n然而，感知器却不能实现异或运算，如下图所示，异或运算不是线性的，你无法用一条直线把分类0和分类1分开。\n\n![](/static/upload-images.jianshu.io/upload_images/2256672-9b651d237936781c.png)\n\n### 感知器的训练\n\n现在，你可能困惑前面的权重项和偏置项的值是如何获得的呢？这就要用到感知器训练算法：将权重项和偏置项初始化为0，然后，利用下面的 **感知器规则** 迭代的修改 $w_i$ 和 $b$ ，直到训练完成。\n\n$$\n\\begin{aligned}\n    w_i &\\gets w_i + \\Delta w_i  &\\pod{12} \\\\\n    b &\\gets b + \\Delta b  &\\pod{13}\n\\end{aligned}\n$$\n\n其中: \n\n$$\n\\begin{aligned}\n    \\Delta w_i &= \\eta(t-y)x_i  &\\pod{14} \\\\\n    \\Delta b &= \\eta(t-y)  &\\pod{15}\n\\end{aligned}\n$$\n\n $w_i$ 是与输入 $x_i$ 对应的权重项， $b$ 是偏置项。事实上，可以把 $b$ 看作是值永远为1的输入 $x_b$ 所对应的权重。 $t$ 是训练样本的`实际值`，一般称之为`label`。而是 $y$ 感知器的输出值，它是根据 **公式(1)** 计算得出。 $\\eta$ 是一个称为 **学习速率** 的常数，其作用是控制每一步调整权的幅度。\n\n每次从训练数据中取出一个样本的输入向量 $\\mathrm{x}$ ，使用感知器计算其输出 $y$，再根据上面的规则来调整权重。每处理一个样本就调整一次权重。经过多轮迭代后（即全部的训练数据被反复处理多轮），就可以训练出感知器的权重，使之实现目标函数。\n\n#### 编程实战：实现感知器\n\n> 完整代码请参考GitHub: https://github.com/hanbt/learn_dl/blob/master/perceptron.py (python2.7)\n\n对于程序员来说，没有什么比亲自动手实现学得更快了，而且，很多时候一行代码抵得上千言万语。接下来我们就将实现一个感知器。\n\n下面是一些说明：\n\n- 使用python语言。python在机器学习领域用的很广泛，而且，写python程序真的很轻松。\n- 面向对象编程。面向对象是特别好的管理复杂度的工具，应对复杂问题时，用面向对象设计方法很容易将复杂问题拆解为多个简单问题，从而解救我们的大脑。\n- 没有使用numpy。numpy实现了很多基础算法，对于实现机器学习算法来说是个必备的工具。但为了降低读者理解的难度，下面的代码只用到了基本的python（省去您去学习numpy的时间）。\n\n下面是感知器类的实现，非常简单。去掉注释只有27行，而且还包括为了美观（每行不超过60个字符）而增加的很多换行。\n\n```python\nclass Perceptron(object):\n    def __init__(self, input_num, activator):\n        '''\n        初始化感知器，设置输入参数的个数，以及激活函数。\n        激活函数的类型为double -> double\n        '''\n        self.activator = activator\n        # 权重向量初始化为0\n        self.weights = [0.0 for _ in range(input_num)]\n        # 偏置项初始化为0\n        self.bias = 0.0\n    def __str__(self):\n        '''\n        打印学习到的权重、偏置项\n        '''\n        return 'weights\\t:%s\\nbias\\t:%f\\n' % (self.weights, self.bias)\n    def predict(self, input_vec):\n        '''\n        输入向量，输出感知器的计算结果\n        '''\n        # 把input_vec[x1,x2,x3...]和weights[w1,w2,w3,...]打包在一起\n        # 变成[(x1,w1),(x2,w2),(x3,w3),...]\n        # 然后利用map函数计算[x1*w1, x2*w2, x3*w3]\n        # 最后利用reduce求和\n        return self.activator(\n            reduce(lambda a, b: a + b,\n                   map(lambda (x, w): x * w,  \n                       zip(input_vec, self.weights))\n                , 0.0) + self.bias)\n    def train(self, input_vecs, labels, iteration, rate):\n        '''\n        输入训练数据：一组向量、与每个向量对应的label；以及训练轮数、学习率\n        '''\n        for i in range(iteration):\n            self._one_iteration(input_vecs, labels, rate)\n    def _one_iteration(self, input_vecs, labels, rate):\n        '''\n        一次迭代，把所有的训练数据过一遍\n        '''\n        # 把输入和输出打包在一起，成为样本的列表[(input_vec, label), ...]\n        # 而每个训练样本是(input_vec, label)\n        samples = zip(input_vecs, labels)\n        # 对每个样本，按照感知器规则更新权重\n        for (input_vec, label) in samples:\n            # 计算感知器在当前权重下的输出\n            output = self.predict(input_vec)\n            # 更新权重\n            self._update_weights(input_vec, output, label, rate)\n    def _update_weights(self, input_vec, output, label, rate):\n        '''\n        按照感知器规则更新权重\n        '''\n        # 把input_vec[x1,x2,x3,...]和weights[w1,w2,w3,...]打包在一起\n        # 变成[(x1,w1),(x2,w2),(x3,w3),...]\n        # 然后利用感知器规则更新权重\n        delta = label - output\n        self.weights = map(\n            lambda (x, w): w + rate * delta * x,\n            zip(input_vec, self.weights))\n        # 更新bias\n        self.bias += rate * delta\n```\n\n接下来，我们利用这个感知器类去实现`and`函数。\n\n```python\ndef f(x):\n    '''\n    定义激活函数f\n    '''\n    return 1 if x > 0 else 0\ndef get_training_dataset():\n    '''\n    基于and真值表构建训练数据\n    '''\n    # 构建训练数据\n    # 输入向量列表\n    input_vecs = [[1,1], [0,0], [1,0], [0,1]]\n    # 期望的输出列表，注意要与输入一一对应\n    # [1,1] -> 1, [0,0] -> 0, [1,0] -> 0, [0,1] -> 0\n    labels = [1, 0, 0, 0]\n    return input_vecs, labels    \ndef train_and_perceptron():\n    '''\n    使用and真值表训练感知器\n    '''\n    # 创建感知器，输入参数个数为2（因为and是二元函数），激活函数为f\n    p = Perceptron(2, f)\n    # 训练，迭代10轮, 学习速率为0.1\n    input_vecs, labels = get_training_dataset()\n    p.train(input_vecs, labels, 10, 0.1)\n    #返回训练好的感知器\n    return p\nif __name__ == '__main__': \n    # 训练and感知器\n    and_perception = train_and_perceptron()\n    # 打印训练获得的权重\n    print and_perception\n    # 测试\n    print '1 and 1 = %d' % and_perception.predict([1, 1])\n    print '0 and 0 = %d' % and_perception.predict([0, 0])\n    print '1 and 0 = %d' % and_perception.predict([1, 0])\n    print '0 and 1 = %d' % and_perception.predict([0, 1])\n```\n\n将上述程序保存为perceptron.py文件，通过命令行执行这个程序，其运行结果为：\n\n![](/static/upload-images.jianshu.io/upload_images/2256672-1e66158656366b57.png)\n\n神奇吧！感知器竟然完全实现了`and`函数。读者可以尝试一下利用感知器实现其它函数。\n\n## 小结\n终于看（写）到小结了...，大家都累了。对于零基础的你来说，走到这里应该已经很烧脑了吧。没关系，休息一下。值得高兴的是，你终于已经走出了深度学习入门的第一步，这是巨大的进步；坏消息是，这仅仅是最简单的部分，后面还有无数艰难险阻等着你。不过，你学的困难往往意味着别人学的也困难，掌握一门高门槛的技艺，进可糊口退可装逼，是很值得的。\n\n下一篇文章，我们将讨论另外一种感知器： **线性单元** ，并由此引出一种可能是最最重要的优化算法： **梯度下降** 算法。\n\n## 参考资料\n\n1. Tom M. Mitchell, \"机器学习\", 曾华军等译, 机械工业出版社\n","tags":["深度学习入门"],"categories":["转载"]},{"title":"解决 \"Failed to Initialize NVML: Driver/library Version Mismatch\"","url":"%2F2019%2F04%2F24%2Fresolve-NVML-driver-version-mismatch%2F","content":"\n服务器更新 `nvidia driver` 后遇到以下问题:\n\n`Failed to initialize NVML: Driver/library version mismatch`\n\n## 一句话解决方案:\n\n```bash\n    # su 权限\n    lsmod | grep -i ^nvidia | awk '{print $1}' | rmmod && nvidia-smi\n```\n\n或者\n\n```bash\n    # 雾\n    sudo reboot\n```\n\n## 原因分析:\n\n驱动更新后 linux 内核对应驱动的 kernel module 并没有重置, 外部相关进程引用了旧版本驱动相关的 mod, 需要手动卸载, 重新执行 `nvidia-smi`\n会自动加载新版本 mod 到内核\n\n## 注意\n\n卸载过程可能会因为相关进程引用或者内核 mod 引用顺序导致卸载失败, 这时需要按照提示顺序卸载.\n\n<!-- more -->\n\n比如:\n\n```bash\n    rmmod nvidia\n    > rmmod: ERROR: Module nvidia is in use by: nvidia_modeset nvidia_uvm\n```\n\n这时就需要先卸载`nvidia_modeset` 和 `nvidia_uvm`\n\n一些相关的 `kernel mod` 命令\n\n* 查看进程引用 `mod`\n\n```bash\n    lsof -n -w /dev/nvidia\n```\n\n* kernel mod 卸载\n\n```bash\n    rmmod <module_name> | modprobe -r <module_name>\n```\n\n* kernel mod 加载\n\n```bash\n    modprobe\n```\n \n\n## 参考资料\n\n- [stackoverflow](https://stackoverflow.com/questions/43022843/nvidia-nvml-driver-library-version-mismatch)\n- [Comzyh的博客](https://comzyh.com/blog/archives/967/)\n- [archlinux](https://wiki.archlinux.org/index.php/Kernel_module)\n","tags":["nvidia"]},{"title":"支持 GPU 调度的 Kubernetes 部署方案(CentOS)","url":"%2F2019%2F04%2F16%2Finstall-k8s-cluster-with-gpu-support%2F","content":"## docker installation\n\n- optional: clean old version if needed\n```\nsudo yum remove docker \\\n                  docker-client \\\n                  docker-client-latest \\\n                  docker-common \\\n                  docker-latest \\\n                  docker-latest-logrotate \\\n                  docker-logrotate \\\n                  docker-engine \\\n                  docker-ce \\\n                  docker-ce-cli \\\n                  containerd.io\n```\n\n- install yum utils\n```\nsudo yum install -y yum-utils device-mapper-persistent-data lvm2\n```\n\n- add docker-ce repo\n```\nsudo yum-config-manager \\\n    --add-repo \\\n    https://download.docker.com/linux/centos/docker-ce.repo\n```\n\n<!-- more -->\n\n- install docker-ce\n```\nsudo yum install docker-ce docker-ce-cli containerd.io\n```\n\n- optional: setup docker `data-root`\n\n`dockerd` store `images/caches/volumes ...` data in `/var/lib/docker` by default, and the `kuberntes` will GC docker\nimage NOT CURRENT IN USING, change the `data-root` to a large disk portion.\n\n```\nsudo vi /usr/lib/systemd/system/docker.service\n\n> append --data-root <a large disk portion> behind dockerd Exec\n```\n\n## nvidia-docker | nvidia-container-runtime installation\n\n- add nvidia-docker repo\n```\ndistribution=$(. /etc/os-release;echo $ID$VERSION_ID)\ncurl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.repo | \\\n  sudo tee /etc/yum.repos.d/nvidia-docker.repo\n```\n\n- install nvidia-docker\n```\nsudo yum install nvidia-docker2\nsudo pkill -SIGHUP dockerd\n```\n\n- modify `/etc/docker/daemon.json` to enable `nvidia` as default docker runtime\n\n- optional: setup your own `shadowsocks server & client & privoxy`\n\n- modify `/usr/lib/systemd/system/docker.server` to enable docker image pull access to `gcr.io`\n\n```\nEnvironment=\"HTTP_PROXY=x.x.x.x:xx;HTTPS_PROXY=x.x.x.x:xx;NO_PROXY=x.x.x.x:xx\"\n```\n\n## kubernetes stack installation (local kubelet)\n\n- optional: remove outdated kubeadm, kubelet, kubectl\n\n```\nsudo yum remove -y kubeadm kubelet kubectl\n```\n\n- `kubelet`, `kubectl`, `kubeadm` install\n    >follow [here](https://kubernetes.io/docs/setup/independent/install-kubeadm/)\n\n- using `kubeadm` to install `HA` cluster\n    >follow [here](https://kubernetes.io/docs/setup/independent/setup-ha-etcd-with-kubeadm/)\n\n## kubernetes stack installation (rke -> stack in docker)\n\n- install [rke](https://github.com/rancher/rke)\n\n- rke up\n\n```yaml\n\nnodes:\n    - address: 192.168.1.14\n      user: jinyi\n      role:\n        - controlplane\n        - etcd\n        - worker\n    - address: 192.168.1.15\n      user: jinyi\n      role:\n        - controlplane\n        - etcd\n        - worker\n    - address: 192.168.1.16\n      user: jinyi\n      role:\n        - controlplane\n        - etcd\n        - worker\n\n```\n\n- more config [here](https://rancher.com/docs/rke/latest/en/)\n\n## apply services & conf to cluster\n\n### kubernetes-dashboard\n\n- apply stable `kubernetes-dashboard`\n\n```bash\n    kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v1.10.1/src/deploy/recommended/kubernetes-dashboard.yaml\n```\n\n- create `admin role binding` (local only for security)\n\n```bash\necho `\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: admin-user\n  namespace: kube-system\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: admin-user\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: cluster-admin\nsubjects:\n- kind: ServiceAccount\n  name: admin-user\n  namespace: kube-system\n` | kubectl apply -f -\n```\n\n- get `dashboard login token` & login to dashboard\n\n```bash\n    kubectl -n kube-system describe secrets admin-user | grep token:\n    \n    # copy the output token to clipboard\n    \n    # start local proxy\n    kubectl proxy\n    \n    # open in bro\n```\n\n- open in browser [kubernetes-dashboard](http://localhost:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/)\n\n- enter `token` you copy before & login\n","tags":["kubernetes"]},{"title":"Css中清除浮动的几种方式","url":"%2F2016%2F10%2F26%2Fclear-float-of-ul%2F","content":"\n前端使用 `ul > li` + `float` 方式生成一个 `navbar` 是一种常见的页面展示手段, 但是浮动之后会导致`ul`高度无法正常撑起, 所以需要清除浮动以正常撑起父元素高度. 这里介绍几种常见的浮动清除的方式.\n\n```html\n    <ul>\n        <li></li>\n        <li></li>\n        <li></li>\n        <li></li>\n    </ul>\n```\n\n```css\n    ul {\n        margin: 0 0;\n        padding: 0 0;\n        list-style-type: none;\n    }\n\n    li {\n        float: right;\n        width: 80px;\n        height: 40px;\n        margin-right: 5%;\n        margin-bottom: 10px;\n        line-height: 40px;\n        text-align: center;\n    }\n```\n\n<!-- more -->\n\n### 给ul添加高度\n\n    这个是最直接的方法, 给`ul`元素添加一个高度\n\n    ```css\n        ul {\n            height: 40px;\n        }\n    ```\n\n### 给最后一个li后添加一个 **空的** `div`, 给`div`添加`clear: both`样式\n\n    ```html\n        <li>\n        </li>\n        <div style=\"clear:both;\"></div>\n    ```\n\n### 给ul添加`overflow: hidden; zoom: 1`样式\n\n    ```css\n        ul {\n            overflow: hidden;\n            zoom: 1;\n        }\n    ```\n\n### 使用 ul **伪类** 进行浮动清除, 对`ul`添加`class=\"clearfix\"`\n\n    ```css\n        .clearfix {\n            *zoom: 1;\n        }\n        .clearfix:before, .clearfix:after {\n            display: table;\n            line-height: 0;\n            content: \"\";\n        }\n        .clearfix:after {\n            clear: both;\n        }\n    ```\n\n#### 参考链接\n* [推酷](http://www.tuicool.com/articles/3iuaMzn)\n","tags":["浮动"]},{"title":"Nodejs版本更新记录","url":"%2F2016%2F10%2F19%2Fnodejs-versions-update-mark-md%2F","content":"\nv6今天LTS, 官方pending了半个多月今天终于up了. 记录一下node主要版本更新内容, 方便选择. 关于官方进度及相关版本计划可以参考[这里](https://github.com/nodejs/lts), 看起来现在用`V4` 和 `V6` 是明智的, `V5`还是放弃吧.\n\n## v4.x\n\n```\n    v4 更新\n    1. 模板字符串\n    2. 类语法糖\n    3. 箭头函数\n    4. 对象字面量\n    5. Promise\n    6. 新的字符串方法\n    7. let 和 const\n```\n\n## v6.x\n\n```\n    nodejs更新主要新特性\n    1. 默认函数参数\n    2. 展开操作符\n    3. 解构赋值\n    4. new.target\n    5. Proxy, 原生对象\n    6. Reflect, 原生对象\n    7. Symbol, 原生对象\n```\n\n\n### 参考资料\n* [v4](http://wwsun.github.io/posts/upgrade-to-node-v4.html)\n* [v6](http://www.tuicool.com/articles/bqmiU3q)\n"},{"title":"Linux服务器安全设置","url":"%2F2016%2F03%2F18%2Flinux-server-setting%2F","content":"\n简明的linux服务器安全设置指南, 包括: 公钥登录, 禁止密码登录, 禁用 root 账户等.\n\n公司的阿里云主机常年被 ssh 外加 http 各种扫, 除了一方面写出更加安全, 健壮的代码之外, 另一方面服务器的安全设置也不容忽视.\n下面是我自己常用的服务器端相关配置. 阿里云主机, centOS 6.x.\n\n## 账户设置\n\n添加公共账户, 避免直接使用 root 账户.\n阿里云的主机默认只提供了一个 root 账户, 我们需要添加一个工作账户, 并赋予 root 权限, 避免直接使用 root.\n\n```sh\n    useradd devops         //添加 devops 账户\n    passwd devops          //修改 devops 账户密码\n    useradd -G root devops //添加 devops 到 root 用户组\n```\n\n这样我们就拥有了一个 root 权限的账户, 接下来就是禁止 root 账户的 shell 登录和使用.\n\n<!-- more -->\n由于我们以后不会再使用密码登录, 并且要禁止 root 的 shell 登录. 所以, 在禁用之前, 需要先配置好公钥文件, 防止无法正常登录服务器.\n\n1. 生成密钥(ssh-keygen)\n2. 复制公钥到服务器(ssh-copy-id)\n3. 修改 ssh server 配置文件, 允许公钥认证, sudo vi /etc/ssh/sshd_config\n```\n    RSAAuthentication yes       //开启RSA 及公钥认证\n    PubkeyAuthentication yes\n```\n\n4. 修改服务器端文件夹的拥有者及权限, 权限设置是必须的, 否则不能正常识别公钥\n```sh\n    chown -R devops:devops .ssh         //修改.ssh 文件夹的拥有者\n    chmod 700 .ssh                      //修改文件夹权限为700,必须\n    chmod 600 .ssh/authorized_keys      //修改文件权限为600,必须\n    sudo services sshd restart          //重启 ssh 服务\n```\n\n接下来进行 ssh 登录测试, 如果正常登录且未提示输入密码, 证明我们的公钥配置已经生效, 这个时候就可以大胆的关闭 root 账户登录和账户登录的密码验证了.\nsudo vi /etc/ssh/sshd_config\n\n```sh\n    PermitRootLogin no          //禁止 root 用户登录\n    PasswordAuthentication no   //禁用密码验证\n```\n\n## 端口设置及 iptables\n\n除了做到以上的还不能确保足够的安全, 我们需要对服务器的端口进行限定开放.\n\n我司服务器目前对外开放端口只有80, 443, 22三个端口, 即除了 ssh, http, https 之外, 不对外部开放任何端口.有需要可以修改 ssh 默认端口号, sudo vi /etc/ssh/sshd_config\n```\n    port xx\n```\n\n这个配置可以在阿里云的控制面板内进行设置, 当然本地进行设置也是一样的道理.\n\n由于阿里云的机房内不是一台机器, 也就是说尽管我们的主机没有暴露在外网环境下, 但是阿里云的内网内部还是可以扫描到我们的服务器的.所以,\n我们还需要使用 iptables 对内网 ip 进行限制.\n\n我司在阿里有两台服务器, 分布在同一个内网环境, 所以除了两者之间互访之外, 屏蔽所有其他的内网互访. 这个在阿里云主机的控制面板也是可以设置的, iptables 同理.\n\n## 其他\n\n防火墙保持常开, 定时备份, 磁盘加密, 及时查 ssh 和 http 的相关 log, 发现可疑情况及时处理.\n另, 对于 http 层可以在 nginx 接入层设置 ip 黑名单进行屏蔽.\n","tags":["linux"]},{"title":"Nginx配置访问限制","url":"%2F2016%2F03%2F17%2FNginx-secret-limit-setting%2F","content":"\n使用 ngxin 时添加IP访问控制是常见需求, 最近遇到该需求, 简单记录如下.\n\n## 模块依赖\n nginx 的 IP 访问控制依赖于内置的 `ngx_http_access_module`. 在默认情况下, 这个包在编译中是安装的, 除非编译过程中显示指定`--without ngx_http_access_module`.\n\n## 配置方法\n\n样例配置如下, example.conf\n```nginx\nlocation / {\n    deny  192.168.1.1;\n    allow 192.168.1.0/24;\n    allow 10.1.1.0/16;\n    allow 2001:0db8::/32;\n    deny  all;\n}\n\n```\n\n<!-- more -->\n\n写完之后 include 到配置文件即可\n`include /path/to/your_conf/*.conf;`\nPS: 记得加分号, `;`\n\nnginx 对于匹配顺序有如下规定: 任何一个 IP 地址, 访问了该 location 之后, 则`从上到下`对规则进行匹配, `漏斗`原则~ . ~ .\n举例如下:\n\n- `192.168.1.1`: 进入之后匹配到第一个`IP/IP 段`, 则执行对应的规则 `deny`, 返回.\n- `192.168.1.12`: 进入之后第一个未匹配, 跳过; 匹配到第二个, 执行对应规则 `allow`, 返回.\n- `172.168.1.101`: 进入之后前四个都未匹配到, 匹配最后一个 `all`, 执行 `deny`, 返回.\n\n## 配置语法\n\n```\n    Syntax: (allow|deny) address | CIDR | unix: | all\n    Default: —\n    Context: http, server, location, limit_except\n```\n\n语法设置可以为 `allow|deny` 两个关键字, 后面的对应属性可以为 IP 地址/地址段(可以使ipv4 或者 ipv6), CIRD(见[附1](#tip1)), `unix:`, `all`, 无默认值.\n配置模块可以出现在 nginx 的 `http{}`, `server{}`, `location{}`, `limit_except{}` 模块.\n如果属性值配置为`unix:`, 则会允许或拒绝所有 Unix 域socket.(该选项只在 nginx 1.5.1之后的版本生效)\n\n## 可用的第三方配置模块\n nginx 只提供了简单的静态 IP 控制, 不过在服务端接入层用于做长效黑名单控制已经足够了, 如果针对访问控制进行动态规则调整, 我找到了`这货`\n [ngx_white_black_list](https://github.com/codehunte/ngx_white_black_list/blob/master/white_black_list.txt')\n\n\n简单说到这里, 其实还是挺简单的, 注意理一下匹配规则.\n\n* [附1](#tip1):\n\n`无类别域间路由`\n\n>无类别域间路由（Classless Inter-Domain Routing、CIDR）是一个用于给用户分配IP地址以及在互联网上有效地路由IP数据包的对IP地址进行归类的方法。\n在域名系统出现之后的第一个十年里，基于分类网络进行地址分配和路由IP数据包的设计就已明显显得可扩充性不足 （参见RFC 1517）。为了解决这个问题，互联网工程工作小组在1993年发布了一新系列的标准——RFC 1518和RFC 1519——以定义新的分配IP地址块和路由IPv4数据包的方法。\n一个IP地址包含两部分：标识网络的前缀和紧接着的在这个网络内的主机地址。在之前的分类网络中，IP地址的分配把IP地址的32位按每8位为一段分开。这使得前缀必须为8，16或者24位。因此，可分配的最小的地址块有256（24位前缀，8位主机地址，28=256）个地址，而这对大多数企业来说太少了。大一点的地址块包含65536（16位前缀，16位主机，216=65536）个地址，而这对大公司来说都太多了。这导致不能充分使用IP地址和在路由上的不便，因为大量的需要单独路由的小型网络（C类网络）因在地域上分得很开而很难进行聚合路由，于是给路由设备增加了很多负担。\n\n<\n\n>无类别域间路由是基于可变长子网掩码（VLSM）来进行任意长度的前缀的分配的。在RFC 950（1985）中有关于可变长子网掩码的说明。CIDR包括：\n指定任意长度的前缀的可变长子网掩码技术。遵从CIDR规则的地址有一个后缀说明前缀的位数，例如：192.168.0.0/16。这使得对日益缺乏的IPv4地址的使用更加有效。\n将多个连续的前缀聚合成超网，以及，在互联网中，只要有可能，就显示为一个聚合的网络，因此在总体上可以减少路由表的表项数目。聚合使得互联网的路由表不用分为多级，并通过VLSM逆转“划分子网”的过程。\n根据机构的实际需要和短期预期需要而不是分类网络中所限定的过大或过小的地址块来管理IP地址的分配的过程。\n因为在IPv6中也使用了IPv4的用后缀指示前缀长度的CIDR，所以IPv4中的分类在IPv6中已不再使用。\n\n<\n\n>摘自维基百科: [无类别域间路由]('https://zh.wikipedia.org/wiki/%E6%97%A0%E7%B1%BB%E5%88%AB%E5%9F%9F%E9%97%B4%E8%B7%AF%E7%94%B1')\n","tags":["安全"]},{"title":"2016读书单","url":"%2F2016%2F03%2F10%2F2016-reading-list%2F","content":"\n2016年新年读书计划, 准备深入学习JavaScript 的前端和后端开发, 大致了解一下 web 开发周围的相关知识.\n\n## 网络相关\n  http 协议的深入了解, nginx 开发相关, web 安全相关\n  ```\n    * 图解 http\n    * http 权威指南\n    * 跟我学 Nginx + Lua 开发\n    * Programming in Lua 3ed\n    * 白帽子讲 web 安全\n  ```\n\n## web 前端相关\n  html&css 深入理解, bootstrap 框架, JavaScript 深入学习\n  ```\n    * html5 与 css3 基础教程\n    * css 禅意花园\n    * css 权威指南\n    * 深入理解 bootstrap\n    * JavaScript 高级程序设计(第三版)\n    * JavaScript 启示录\n    * JavaScript 语言精粹\n  ```\n\n<!-- more -->\n\n## nodejs 相关进阶\n  nodejs 底层理解, nodejs 加载 C/C++ addon, 全栈\n  ```\n    * nodejs 权威指南\n    * web 全栈工程师的自我修养\n    * C Primer Plus(第五版)\n    * C++ Primer中文版\n  ```\n\n## 架构设计&软件工程相关\n  web 架构设计, 软件工程管理\n  ```\n    * 人月神话\n    * 程序之美系列(架构之美, 安全之美, 数据之美)\n    * 大型网站技术架构: 核心原理与案例分析\n  ```\n\n## 七周系列\n  七周相关经典\n  ```\n    * 七周七语言\n    * 七周七并发模型\n    * 七周七数据库\n    * 七周七 web 开发框架\n  ```\n","tags":["读书"]}]